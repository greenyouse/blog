#+HTML: <div class="container-fluid"><div class="row"><div class="col-md-7 col-md-offset-3 col-xs-12 col-sm-10 col-sm-offset-1 col-lg-6 col-lg-offset-3">
#+TITLE: Building a Stack
#+HTML: <br><br>

Building systems is usually seen as a hard topic for senior people only
but it's really something that every engineer should take a hard look
at. By writing programs we are all building some component of a larger
system. This begs the question, why not take the time to focus on
engineering the system itself? Are there ways that we can apply program
level constructs to system design? What would it be like to create
an entire system?

* Systems 101

  Last month I wrapped up projects for my Senior Web Developer Nanodegree
  from Udacity and had a free month to work on whatever I wanted. OK, I
  could have jumped straight into the job market but I wanted to dig into
  distributed systems while I had the chance. How many times on the job
  does an engineer get to build an entire system? Likely none. There's
  usually already a system in place or, at best, you'll work with other
  people and be left with a subset of the system that you own. That's OK
  but how would someone design a system given the opportunity? What kind
  of stuff could one learn to get ready for it? What kind of tricks
  could you pick up along the way?

#+HTML: <br>

  As you're probably getting the hint, this was way too tempting not to
  investigate. :-)

#+HTML: <br>

  To begin, I tried categorizing common parts of a system. This won't be
  universal since different systems have different objectives but here's a
  basic list:

- Front-end: Web or App interface to a service
- Back-end: App servers, Web servers, Microservices, 3rd Party, etc.
- Components: Tools for caching, queuing, load balancing, etc.
- Database: How info gets processed and stored long-term and short-term
- Analytics: Data Science things
- OAM: Operations, Administration, and Management
- Computing Platform: Physical servers, OS, Cloud Platform, etc.

#+HTML: <br>

I've seen some people talk about how putting these parts together is
like snapping LEGO blocks together and magically ending up with a
system. While that would probably work for a mental model, that doesn't
quite carry over to the real world. Much like how programs have errors
and error handling, systems have errors too (network outages, hardware
spontaneously busting, cats accidentally pulling cords out) and the
system has to handle whatever can be thrown at it.

#+HTML: <br>

* Errors

  The basic trade-off for error handling is described by [[https://en.wikipedia.org/wiki/Cap_theorem][CAP
  theorem]]. The gist is that out of the three options of consistency,
  availability, or partition tolerance, a system can only have two of
  the three. In practice, partition tolerance is always assumed because
  people host out of stable data centers that have good uptimes. That
  leaves a choice between capacity and availability. I think many
  systems end up somewhere in between the two by using eventual
  consistency via CRDTs, [[https://en.wikipedia.org/wiki/Quorum_(distributed_computing)][quorums]], client-side storage, and other techniques.

#+HTML: <br>

  (Well, people don't always really get partition tolerance because they
  usually don't use redundant services to host their systems. It ends up
  costing more and the SLAs for hosting usually promise one or two
  nines, which is good enough. That said, I've seen major hosting
  providers have multi-day outages for a zone which crashed lots of web
  services. Just a few weeks ago Dyn DNS went out along with all the
  services that used it as a single point of failure: Twitter, GitHub,
  Reddit, Paypal, Spotify, and others. Having redundant DNS or hosting
  providers could have avoided the problems but it's a design decision
  for their system: cost vs fault tolerance.)

#+HTML: <br>

  There's also the case of acute errors like when an HDD breaks. To
  recover from these kinds of problems, the OAM layer should be
  monitoring the server's health and handle the problem
  automatically. The general idea is that the server will be marked as
  down due to hardware failure, the traffic will be distributed to other
  servers, and a notification will go out to a human that's on call
  somewhere. Runtime exceptions and other normal problems should be
  written to a log and possibly ping a team member if it demands attention.

#+HTML: <br>

* Scaling

  Designing for scale is another tricky challenge. There are usually two
  methods of scaling a system: vertically (adding more cores per node)
  or horizontally (adding more nodes to a network). Some programming
  languages definitely scale better than others but pretty much any
  language can be Franken-programmed into scaling both vertically and
  horizontally nowadays. Single threaded servers like Node.js can be
  hosted with IIS so that each computer core is given an instance of the
  server program to run. Load balancers can have different methods to
  partition the network load across nodes for horizontal scaling.

#+HTML: <br>

  [[https://www.youtube.com/watch?v=xrIjfIjssLE][Erlang]], being the language of distributed systems, is the only
  language I'm aware of that can do both horizontal and vertical
  scaling. Similar to how Java or C has threads, Erlang has sandboxed
  processes. The difference is that a normal computer can support
  millions of concurrent Erlang processes and networks can distribute
  the processes transparently across nodes. There are lots of other
  language features that make [[https://www.youtube.com/watch?v=rRbY3TMUcgQ][Erlang]] great for building systems in
  too. For more, I'd highly suggest reading:

- [[http://erlang.org/pipermail/erlang-questions/2014-November/081570.html][this post]]
- [[http://erlang.org/download/armstrong_thesis_2003.pdf][the original white paper]]
- [[http://learnyousomeerlang.com/content][Learn You Some Erlang for Great Good]]
- [[https://www.amazon.com/Designing-Scalability-Erlang-OTP-Fault-Tolerant/dp/1449320732][Designing for Scalability with Erlang/OTP: Implement Robust, Fault-Tolerant Systems]]

#+HTML: <br>

  Other tools like CDNs, Redis, or Memcache are great for caching static
  content for web servers too.

#+HTML: <br>

* Database

  There are also all kinds of database problems that happen when scaling a
  system. There are basic SQL vs NoSQL arguments that have played out
  ad nausem. Then there are complete paradigm shifts like Big Data systems
  which use an immutable database to remember everything that
  happens. Databases need to be partitioned among network clusters once
  they get big enough and they need to take into account CAP theorem
  requirements too (e.g. ACID vs BASE vs trade-offs).

#+HTML: <br>

  The Big Data systems take the approach of an immutable database for a
  variety of reasons that can make systems. Here's a general list of pros
  and cons for using an immutable database.

#+HTML: <br>

  benefits of immutable databases:
  - temporal analysis of things (time series analysis)
  - easier to run analysis on data retroactively (everything is open to
    analysis, not just key metrics that we care about right now. Take a
    snapshot and analyze everything. Audits!)
  - aggregating data
  - reads can always be done (no read/write synchronization needed since
    all data is immutable)
  - fast writes
  - fast reads (especially with in-memory databases)
  - easier to handle scale + fault tolerance with database replication
  - choice of underlying database (each has different attributes like
    read/write speed or scalability)

#+HTML: <br>

  disadvantages:
  - more storage space (but space is so cheap right now, right?)
  - fewer learning resources
  - can be hard to set up if you want lots of features (like try writing
    your own Datomic with concurrent transactors that can be tuned for
    consistency/availability on top of Riak clusters that also has no
    datom limit)

#+HTML: <br>

  I'm personally a fan of Nathan Marz's Lambda Architecture for Big Data
  systems. There's a good [[https://www.manning.com/books/big-data][book]] about it from Manning and a nice
  [[https://www.youtube.com/watch?v=ucHjyb6jv08][talk here]] (there are other good talks about this too). The main
  trade-off here would be a larger learning curve but using Clojure
  makes it a little easier at least. In practice, I think a "do things
  that don't scale" solution is usually made to get an MVP off the
  ground. That usually means making painful, in-flight database updates
  to improve the database's latency and throughput as the business
  grows. Finally, somewhere down the line, the database becomes scalable
  and work on it can switch into maintenance mode.

#+HTML: <br>

  In the spirit of Rich Hickey's [[https://www.infoq.com/presentations/Simple-Made-Easy][simple made easy talk]], it might
  be better to just eat the upfront development time to learn how to
  build Big Data systems (or at least how to engineer a scalable
  solution for the database). This is pretty generic and works for most
  startup ideas you'll want to try banging out. The long-term
  maintenance cost is much lower, which frees up more time to work on
  cool stuff or pressing business problems.

#+HTML: <br>

* Analytics

  I'm not much of a data science person but from what I've seen there
  are some tricks that could be used to wring out more info from a
  dataset. A new MIT project called the [[http://dai.lids.mit.edu/Pred_eng.pdf][Data Science Machine]] looks like
  an interesting approach for creating generic predictions. It promises
  to automate much of the work that a data scientist would normally be
  hired for.

#+HTML: <br>

  The general idea is that a dataset will be indexed by time and a
  series of functions will form a pipeline that automates much of the
  data analysis. There's a [[https://www.youtube.com/watch?v=d4f1jzhUjjs][nice video set]] (complete with really
  distracting music) that digs into more detail.

#+HTML: <br>

  This is another place where keeping an immutable, master dataset will
  pay off. In most small startups the analytics system is designed to
  capture some key metrics and stash them in a database for analysis
  later. With a Big Data style system everything can be analyzed with
  time series so nothing is overlooked. That leads to better predictive
  analytics and maybe better ways to monetize data for 3rd parties
  depending on the industry.

#+HTML: <br>

* Components

  This part of the system varies widely depending on what domain the
  system is for. A good rule might be to treat system components like
  stateless functions. They should be reusable parts to transform
  values, route values, cache values, or move values. This could be
  something like a queue that moves data around and decouples the input
  and output processes. RabbitMQ is an example of queue that does just
  that. A router example might be an event processor like Storm.

#+HTML: <br>

  The point is that each part is abstract, composable, uses messaging
  protocols for its communication (message sequence diagrams help), and
  is reusable.

#+HTML: <br>

  Another example might be a database abstraction layer that can run
  over popular databases. That would allow a team to pick a cheap but
  limited database to start out and easily migrate to an enterprise
  database later. As long as there's an abstract interface over the
  database, the app will never know which database we're actually
  building on top of.

#+HTML: <br>

* Back-end

  Servers should be low latency, high throughput, handle errors, scale,
  and all that stuff. There's not a lot I can add here that isn't
  already known. Clojure is pretty great since it has relatively
  mature tools and access to many Java libraries. I think people
  should just use whatever they're comfortable with as long as the
  performance is decent. Having a concurrent language is definitely an
  added bonus for performance, though.

#+HTML: <br>

* Front-end

  The public interface to your system! Generally, this is either a
  website or an app. Some people are driven to stick with one or the
  other ("apps are the future" or "nuh-uh, HTML5 is the best") but it
  might be better to be indifferent and do both as is necessary. For a
  small company starting out, Progressive Web Apps are probably the path
  of least resistance.

#+HTML: <br>

  For the last half a year I've been working on Progressive Web Apps in
  Udacity's Senior Web Developer Nanodegree program. The general idea is
  to make a web app that can be downloaded to the home screen for mobile
  devices and desktop and have as much parity with native apps as
  possible. The app should have native features like a splash screen on
  startup, push notifications, offline usage, and have good performance
  (no jank). The development time for a PWA is not much more than a
  normal web app but the reach is pretty great! It also has other
  advantages like using less bandwidth for installs (an actual problem
  for developing countries), not needing to be vetted by an app store
  review for every update, and, hey, you end up with an app that can
  install to homescreen on desktops too.

#+HTML: <br>

  Eventually, if a business takes off it would make sense to hunker down
  and write native apps. PWAs aren't really a replacement for that. This
  could be a cheaper way to get rolling though and could be a better
  solution for some developing parts of the world.

#+HTML: <br>

* Computing Platform

  How many options are there for hosting providers? Probably a
  billion. After taking a look at the choices between cloud,
  co-location, and self-hosting, my choice for kicking something off
  would be to just self-host. Minneapolis has 1Gbps or 10Gbps speeds for
  cheap (like my $70/month 6Mpbs connection costs more than
  1Gpbs). It's usually not a good idea to run things
  yourself because power outages and general bad things will probably
  happen.

#+HTML: <br>

  I think Co-location is probably the most reasonable for a longer term
  plan. Once the traffic on a service stabilizes, it is much cheaper to
  get your own hardware and use a co-location centre. Cloud
  hosting can be used to handle traffic spikes but is pretty
  expensive. The obvious exception is if you're in a super disruptive
  startup that does hockey stick growth. Then the cloud hosting option
  would probably be a better fit.

#+HTML: <br>

  I did a little poking around for how to get good server prices and
  eBay seems to be the most reasonable. Buying up used 1U blade servers
  to start with is a pretty good deal. Costs for hardware seem to rise
  exponentially, so the best approach is definitely going with commodity
  hardware.

#+HTML: <br>

  For the OS decision, I think we can make a list of some functional
  requirements:

  - easy to set up
  - doesn't hog resources
  - can update well
  - works with a hosted cloud service (just in case)
  - can handle ZFS
  - is mature and has good support

  The easiest I can think of is using [[https://www.joyent.com/smartos][SmartOS]] since it can run docker
  containers directly on bare metal. SmartOS can be dd-ed onto a flash
  drive and loaded directly into RAM. That's nice because it doesn't run
  off the disk and has a small memory footprint. It's been around for a
  while and is a derivative of Illumos (which comes from OpenSolaris) so
  support is good too. There's a nice description [[https://www.youtube.com/watch?v=dxZExLeJz2I][here]].

#+HTML: <br>

  Basically, getting rid of VM the overhead translates into [[https://mattconnolly.wordpress.com/2012/11/18/comparing-amazon-ec2-to-joyent-smartos/][better]]
  [[https://www.joyent.com/blog/joyent-and-hadoop-making-big-data-better][speeds]]. Joyent also offers a platform for better ops automation
  [[https://www.joyent.com/blog/docker-bake-off-aws-vs-joyent][throughout]].

#+HTML: <br>

  Being a [[https://www.youtube.com/watch?v=mPhjFYXoAD0][cloud native solution]] is really nice for efficiency and more
  importantly system level automation!

#+HTML: <br>

* OAM

  In getting the hang of Erlang over the past month, I noticed that it
  has some awesome tools for administration and management. Here's a list
  of most of the things it can do:

  - an OS heartbeat script to monitor nodes
  - automatically restart processes or full nodes if they become
    unresponsive
  - live code updates and rollbacks
  - an Erlang shell that can jack into any remote processes

#+HTML: <br>

  Kubernetes offers something pretty similar:
  - health monitoring for nodes
  - automatic restart when a node becomes unresponsive
  - handles load balancing for health and readiness of nodes
  - code deploys and rollbacks
  - command line interface

#+HTML: <br>

  These both work great for managing container deploys but not so great
  for system level operations, administration, and management. Luckily,
  [[https://docs.project-fifo.net/][Project FIFo]] and [[https://www.joyent.com/triton][Triton]] are open source projects for handling just
  that on SmartOS systems.

#+HTML: <br>

  I haven't had enough time to really evaluate whether one is better
  than the other. Both look like interesting solutions for managing a
  private cloud datacenter, though.

#+HTML: <br>

  Cloud providers like AWS offer tools for OAM tasks with the obvious
  problem that they only work with their service. There's no option to
  use them in your own datacenter.

#+HTML: <br>

  There are many other tools that work for OAM but they are more complex
  to integrate with SmartOS. There are already enough services provided
  by ProjectFIFo/Triton, Kubernetes, and Docker. Stitching together five
  or six separate tools to try building the same thing would be a large
  time sync and bring more maintenance overhead.

#+HTML: <br>

* Extra Hardware Hacks

  I came up with a few other ideas that were much more experimental while
  playing around with how to build the infrastructure. These were mostly
  solutions for rich man's problems (e.g. needing petabytes of storage)
  and not things to worry about in the early stages of a startup.

#+HTML: <br>

  For storage space, I like the BACKBLAZE team's plan to make a
  [[https://www.backblaze.com/blog/petabytes-on-a-budget-how-to-build-cheap-cloud-storage/][petabyte scale storage server]]. They crammed 45 HDDs into a box and
  added some hardware, which in 2009 only added up to 76TB. Now that we
  have 4TB disks it should be 45 * 4 (180TB). Pretty awesome!

#+HTML: <br>

  Since the storage servers would likely hold the master dataset for
  Hadoop, we could compress things since speed is less important for the
  batch processing layer. By default HDFS (Hadoop filesystem) uses gzip
  for compression so there's already some savings. To add to that, we could
  use a ZFS filesystem and get a bit more compression for cheap
  too. Hopefully, that could significantly bump storage for the server!

#+HTML: <br>

  (Having that amount of storage is definitely a rich man's problem. For
  starting a company, it's obviously better to use minimal storage and see
  where things go.)

#+HTML: <br>

  There are other cool looking hardware hacks with FPGAs that could save
  money too. RAM seems to be an important hardware component that
  limits the system because it's so darn expensive. One way to work
  around that might be to have a ram cloud using
  [[http://dspace.mit.edu/handle/1721.1/97746][BlueDBM]]. The project basically promotes using a rack of SSDs with an
  FPGA as a replacement for RAM. The read time on the system is slightly
  slower than normal DRAM but it consumes less power and is an order of
  magnitude cheaper.

#+HTML: <br>

  If the Lambda Architecture was used, maybe it would be reasonable to
  use a RAM cloud for the serving layer. The serving layer holds a view
  that was created from the master dataset. It's an immutable snapshot,
  so maybe it would be conceivable to store the entire database in the
  RAM cloud using something like Voldemort. Doing that might yield low
  latency reads with higher throughput.

#+HTML: <br>

  (OK, this is a super rich man's problem. Mostly just fun to read and
  think about. This part is super experimental anyway and probably
  wouldn't work.)

#+HTML: <br>

* Conclusion

  This post was a general overview of how to engineer a system
  architecture and some of the design tradeoffs that come with it. I
  tried to keep from talking too much about any one layer to instead lay
  out a rough roadmap of the system. Each part has more detail and lower
  level design decisions that were glossed over. Some topics like
  networking also were skipped to focus on other parts.

#+HTML: <br>

  I didn't talk about other ideas like the [[https://12factor.net/][12 factor app model]] either
  but hopefully you can draw your own parallels between this design and
  that model.

#+HTML: <br>

  Before writing this up I was focusing on Erlang and building a system
  infrastructure for a startup idea. I wanted to write things down
  to see what other people thought. Sorry if parts were more focused on
  my design choices and less on general patterns. My goal was to make a
  generic stack based on the Lambda Architecture that could be reused
  for a few companies.

#+HTML: <br>

  It's fun to try picking which technologies to use and then jumping in
  to work on it. I hope my notes help elucidate some of the process!

#+HTML: <br><br>
#+HTML: <div id="disqus_thread"></div> <script> var disqus_config = function () { this.page.url = "https://edbabcock.com"; this.page.identifier = "building-a-stack"; }; (function() { var d = document, s = d.createElement('script'); s.src = '//edbabcock-com.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); </script> <noscript>It would be better if comments didn't need JS. Turn JavaScript on to see the comments. <a href="https://disqus.com/?ref_noscript">Comments powered by Disqus.</a></noscript>

#+HTML: <br>
#+HTML: </div></div></div>
